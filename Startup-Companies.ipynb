{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd93d774",
   "metadata": {},
   "source": [
    "# Imports & Notes to Improve Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a03a2c46",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup matplotlib to plot inline (within the notebook)\n",
    "%matplotlib inline\n",
    "# Import the pyplot module of Matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "# Import pandas under the abbreviation 'pd'\n",
    "import pandas as pd\n",
    "# Import NumPy under the abbreviation 'np'\n",
    "import numpy as np\n",
    "# Libraries to aid in web scraping\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import seaborn as sns;\n",
    "#Notes for efficienccy:\n",
    "#shift + tab when cursor is in arguments to bring up its documentation\n",
    "#ex: pd.DataFrame(Shift+tab here)\n",
    "#recall: Json == Dictionary\n",
    "#to know functions of pandas:\n",
    "#dir(pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6341c845",
   "metadata": {},
   "source": [
    "# Reading The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ef2f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pandas and read_csv()\n",
    "acquis = pd.read_csv(\"datasets/startups/acquisitions.csv\")\n",
    "additions = pd.read_csv(\"datasets/startups/additions.csv\")\n",
    "companies = pd.read_csv(\"datasets/startups/companies.csv\")\n",
    "invests = pd.read_csv(\"datasets/startups/investments.csv\")\n",
    "rounds = pd.read_csv(\"datasets/startups/rounds.csv\")\n",
    "dfAll = [acquis, additions, companies, invests, rounds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adae982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shapes (rows x cols) of each dataset\n",
    "for ele in dfAll:\n",
    "    print(ele.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaaa3d5",
   "metadata": {},
   "source": [
    "# Samples of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c408c73",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acquis.sample(5).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb7752f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "additions.sample(5).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b125c04b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "companies.sample(5).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9941b585",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "invests.sample(5).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51edfb44",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rounds.sample(5).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad6ebb7",
   "metadata": {},
   "source": [
    "# Seeing the columns in the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba49db80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasetsCols(dfList, datasetNames, pad=\"\"):\n",
    "    \"\"\"\n",
    "    Takes dataframes and returns a dataframe (df) with each column having a df's\n",
    "    column names. The \"pad\" is to make sure all lists of columns have \n",
    "    the same length padded out with \"pad\"\n",
    "    Example\n",
    "    pad = \"XX\", datasetNames = [\"cars\", \"planes\"]\n",
    "    dfList = [carsDF, planesDF]\n",
    "    returned data frame:\n",
    "            cars            planes\n",
    "    0       numOfWheels     numOfWings\n",
    "    1       manufacturer    manufacturer\n",
    "    2       make            XX\n",
    "    \"\"\"\n",
    "    cols = []\n",
    "    maxArrayLen = 0\n",
    "    for i, df in enumerate(dfList):\n",
    "        cols.append(df.columns.tolist())\n",
    "        maxArrayLen = max(maxArrayLen, len(cols[i]))\n",
    "    \n",
    "    dictCsvs = {}\n",
    "    for i, df in enumerate(dfList):\n",
    "        cols[i] += [pad] * (maxArrayLen - len(cols[i])) #padding the lists to make them have equal lengths \n",
    "        dictCsvs.update({datasetNames[i] : cols[i]})\n",
    "\n",
    "    return pd.DataFrame(dictCsvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33af67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstCsvNames = [\"acquisitions\", \"additions\", \"companies\", \"investments\", \"rounds\"]\n",
    "\n",
    "dfAllCols = datasetsCols(dfAll, lstCsvNames)\n",
    "dfAllCols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039bbb29",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54cb0be",
   "metadata": {},
   "source": [
    "First, lets change the columns in the `companies` dataset <br>\n",
    "to be the similar to all other datasets: <br>\n",
    "(we're doing this in case we merge the datasets together, we want them to have the same column names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733c45ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies.rename(columns = {'permalink' : 'company_permalink', \n",
    "                            'name' : 'company_name',\n",
    "                            'category_list' : 'company_category_list',\n",
    "                            'country_code' : 'company_country_code',\n",
    "                            'state_code' : 'company_state_code',\n",
    "                            'region' : 'company_region',\n",
    "                            'city' : 'company_city'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97b0327",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAll[2] = companies\n",
    "dfAllCols = datasetsCols(dfAll, lstCsvNames)\n",
    "dfAllCols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb58f24",
   "metadata": {},
   "source": [
    "Let's start by analyzing `additions` dataset, as it has unusual format of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cccbd3",
   "metadata": {},
   "source": [
    "## Removing \"additions\" Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79950699",
   "metadata": {},
   "outputs": [],
   "source": [
    "additions.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e002f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "additions[\"content\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedd2480",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAllCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4686020f",
   "metadata": {},
   "outputs": [],
   "source": [
    "additions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c839cf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies.shape, rounds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a1cfb6",
   "metadata": {},
   "source": [
    "So apparently there are no columns that could be used to join with the other datasets. <br>\n",
    "Furthermore, the \"value\" column is too vague to be useful. <br>\n",
    "Therefore, the \"additions\" dataset will be discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383f43b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAll.pop(1)\n",
    "lstCsvNames.pop(1)\n",
    "len(dfAll), lstCsvNames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1ae2ef",
   "metadata": {},
   "source": [
    "## `rounds` Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4bc939",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAllCols.iloc[:, -2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d1112e",
   "metadata": {},
   "outputs": [],
   "source": [
    "invests.shape, rounds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b2c309",
   "metadata": {},
   "outputs": [],
   "source": [
    "invests.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920c0c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0108965",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(invests[\"company_name\"].unique()), len(rounds[\"company_name\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dca758b",
   "metadata": {},
   "source": [
    "Since there are companies in `rounds` dataset that are not in `investments` dataset, <br>\n",
    "therefore we should keep `rounds` dataset for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76af470",
   "metadata": {},
   "source": [
    "# Data Cleaning (Cont.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41038222",
   "metadata": {},
   "source": [
    "## Checking All Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1be274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasetsNulls(dfList, datasetNames, nullCol=\"nulls_\", pad=\"\"):\n",
    "    \"\"\"\n",
    "    Use this when you want to display a column of column names,\n",
    "    then a column of the null values, and repeat that for each dataset.\n",
    "    Returns a dataframe\n",
    "    \"\"\"\n",
    "    cols = []\n",
    "    nulls = []\n",
    "    maxArrayLen = 0\n",
    "    for i, df in enumerate(dfList):\n",
    "        cols.append(df.columns.tolist())\n",
    "        nulls.append(df.isnull().sum().tolist())\n",
    "        maxArrayLen = max(maxArrayLen, len(cols[i]))\n",
    "    \n",
    "    dictCsvs = {}\n",
    "    for i, df in enumerate(dfList):\n",
    "        cols[i] += [pad] * (maxArrayLen - len(cols[i])) #padding the lists to make them have equal lengths \n",
    "        nulls[i] += [None] *  (maxArrayLen - len(nulls[i]))\n",
    "        dictCsvs[datasetNames[i]] = cols[i]\n",
    "        dictCsvs[nullCol + str(i+1)] = nulls[i]\n",
    "    \n",
    "    return pd.DataFrame(dictCsvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f3e7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAllNulls = datasetsNulls(dfAll, lstCsvNames)\n",
    "dfAllNulls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326d576c",
   "metadata": {},
   "source": [
    "### `dropna()` for columns with few missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ad80c3",
   "metadata": {},
   "source": [
    "Notice that there is one row that doesn't contain a company name, <br>\n",
    "so let's validate that it is the same company across all datasets, <br>\n",
    "so we can remove it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1927e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetsCols(dfAll, lstCsvNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd94aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#company_permalink of each row that doesn't have a company name:\n",
    "for df in dfAll:\n",
    "    print(df[df['company_name'].isnull()]['company_permalink'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e22df8a",
   "metadata": {},
   "source": [
    "So apparently, the `acquisitions` dataset has a different permalink that has no company name, <br>\n",
    "So we could remove these two companies, as the total number of companies are big enough to get the insight that we want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61b89a7",
   "metadata": {},
   "source": [
    "`dfAllNulls` dataframe also shows that there are missing values for `acquirer_name` and `investor_name` <br>\n",
    "which can't be imputed, so we'll also remove them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754da1f9",
   "metadata": {},
   "source": [
    "Finally, we'll remove the missing values for columns that have less than 100 `NaN`, <br>\n",
    "as they're small in comparison to their respective datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3842278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This syntax won't work, as it turns out, df[0] is a copy of a dataframe (eg: a copy of acquisitions, so they don't share the same reference)\n",
    "#for df in dfAll:\n",
    "#    nulls = df.isnull().sum()\n",
    "#    cols = nulls[(nulls <= 100) & (nulls != 0)].index.tolist()\n",
    "#    df.dropna(subset=cols, inplace=True)\n",
    "#dfAllNulls = datasetsNulls(dfAll, lstCsvNames)\n",
    "#dfAllNulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded2b1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls = acquis.isnull().sum()\n",
    "cols = nulls[(nulls <= 100) & (nulls != 0)].index.tolist()\n",
    "acquis.dropna(subset=cols, inplace=True)\n",
    "\n",
    "nulls.append(companies.isnull().sum()\n",
    "cols.append(nulls[(nulls <= 100) & (nulls != 0)].index.tolist()\n",
    "companies.dropna(subset=cols, inplace=True)\n",
    "\n",
    "nulls = invests.isnull().sum()\n",
    "cols = nulls[(nulls <= 100) & (nulls != 0)].index.tolist()\n",
    "invests.dropna(subset=cols, inplace=True)\n",
    "\n",
    "nulls = rounds.isnull().sum()\n",
    "cols = nulls[(nulls <= 100) & (nulls != 0)].index.tolist()\n",
    "rounds.dropna(subset=cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4734cb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAll = [acquis, companies, invests, rounds]\n",
    "lstCsvNames = [\"acquisitions\", \"companies\", \"investments\", \"rounds\"]\n",
    "dfAllNulls = datasetsNulls(dfAll, lstCsvNames)\n",
    "dfAllNulls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de38a982",
   "metadata": {},
   "source": [
    "## Converting & Imputing Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637ff2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Farah: plz impute (not drop) the columns' missing values related to dates in each dataset which are:\n",
    "#founded_at.. and that's it :] \n",
    "# Suggestion: convert the dates of all the columns that have date values from \"object\" to datetime\n",
    "#       Helpful link: https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#converting-to-timestamps\n",
    "# Then, make the founded_at a couple of days before first_funding_at\n",
    "\n",
    "#Sincerely No. -Farah :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed10657",
   "metadata": {},
   "source": [
    "We are currently interested in 'founded_at', 'first_funding_at' and 'last_funding_at' columns. Before doing operations on dates, they need to be converted first from 'Object' to 'DateTime'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1eddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies['founded_at'] =pd.to_datetime(companies['founded_at'], errors = 'coerce')\n",
    "companies['first_funding_at'] =pd.to_datetime(companies['first_funding_at'], errors = 'coerce') #3 rows will not be converted succesfully and will become \"NaN\" (shown later)\n",
    "companies['last_funding_at'] =pd.to_datetime(companies['last_funding_at'], errors = 'coerce')\n",
    "companies.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f20f55f",
   "metadata": {},
   "source": [
    "After converting, we need to impute the missing values on the dates of when those companies where founded at. So, this will be done by first creating a new column that calculates how long it took to get the first funding. This will be done by subtracting the date of the founding from the date of the first funding. Note that we will find some values in negative which indicate that some companies took funding before establishing it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78930d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies['time_before_first_fund'] = companies['first_funding_at'] - companies['founded_at']\n",
    "companies['time_before_first_fund']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6819874",
   "metadata": {},
   "source": [
    "Now we need to calculate the mean of the time before the first funding and impute the missing values with this average. Notice that the average is 1370 days which is around 3.7 years which means there must be some outliers that messed up the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c622f6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies['time_before_first_fund'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092e391e",
   "metadata": {},
   "source": [
    "Therefore, we will need to visualize those outliers and maybe if they are causing inaccuracy in the data, we can drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac82a773",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(companies['first_funding_at'], companies['founded_at'], s = 20, alpha = 0.5)\n",
    "plt.xlabel('first_funding_at')\n",
    "plt.ylabel('founded_at')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d06573",
   "metadata": {},
   "source": [
    "We Noticed that there are some outliers than can be removed from the dataset so we will create a temporary dataframe that will contain the outliers so we can drop them later on. The first outliers that we want to deduct are the ones at the left of the center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aada3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "tempdf = companies.copy()\n",
    "tempdf = tempdf[(tempdf.first_funding_at <= datetime.strptime(\"01/01/1983\", \"%d/%m/%Y\"))& (tempdf.founded_at <= datetime.strptime(\"01/01/2000\", \"%d/%m/%Y\"))]\n",
    "dropseries = tempdf.copy()\n",
    "dropseries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed79dcce",
   "metadata": {},
   "source": [
    "Plotting the outliers on a separate graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa40870",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tempdf['first_funding_at'], tempdf['founded_at'], s = 20)\n",
    "plt.xlabel('first_funding_at')\n",
    "plt.ylabel('founded_at')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9c6f45",
   "metadata": {},
   "source": [
    "Now we will determine the outliers that are the bottom of the center from the original graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72c0fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdf = companies.copy()\n",
    "tempdf = tempdf[(tempdf.first_funding_at < datetime.strptime(\"01/01/2005\", \"%d/%m/%Y\"))& (tempdf.founded_at <= datetime.strptime(\"01/01/1900\", \"%d/%m/%Y\"))]\n",
    "tempdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44b745a",
   "metadata": {},
   "source": [
    "Plotting the outliers on a separate graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c57f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tempdf['first_funding_at'], tempdf['founded_at'], s = 20)\n",
    "plt.xlabel('first_funding_at')\n",
    "plt.ylabel('founded_at')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1009fd9c",
   "metadata": {},
   "source": [
    "Now we will concatenate the newly discovered outliers to the ones from before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d999df32",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropseries = pd.concat([dropseries, tempdf])\n",
    "dropseries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ea4667",
   "metadata": {},
   "source": [
    "Finally, we will deduct the outliers that are the top right corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51667bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdf = companies.copy()\n",
    "tempdf = tempdf[(tempdf.first_funding_at > datetime.strptime(\"01/01/2010\", \"%d/%m/%Y\"))& (tempdf.founded_at >= datetime.strptime(\"01/01/2030\", \"%d/%m/%Y\"))]\n",
    "tempdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a561ef40",
   "metadata": {},
   "source": [
    "Visualizing the outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eccbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tempdf['first_funding_at'], tempdf['founded_at'], s = 20)\n",
    "plt.xlabel('first_funding_at')\n",
    "plt.ylabel('founded_at')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e1b0e8",
   "metadata": {},
   "source": [
    "Concatinating the outliers on the rest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccda7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropseries = pd.concat([dropseries, tempdf])\n",
    "dropseries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a54c937",
   "metadata": {},
   "source": [
    "Now, we can drop all of the outliers that have been detected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1452df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "list = dropseries.index\n",
    "companies = companies.drop(list)\n",
    "#companies['time_before_first_fund'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd65e6d",
   "metadata": {},
   "source": [
    "Plotting the graph once more after cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfd4c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(companies['first_funding_at'], companies['founded_at'], s = 20, alpha=0.5)\n",
    "plt.xlabel('first_funding_at')\n",
    "plt.ylabel('founded_at')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735aeb21",
   "metadata": {},
   "source": [
    "Now, we can calculate the mean. Notice that there were not much of a difference but deducting the outliers helped keeping the data realistic because some dates were 2090, 2100, etc. which is unrealistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a536c339",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies['time_before_first_fund'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfabcb20",
   "metadata": {},
   "source": [
    "Filling the missing values with the new mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9ac258",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies['time_before_first_fund'].fillna(companies['time_before_first_fund'].mean(), inplace = True)\n",
    "companies['time_before_first_fund']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c55e6b",
   "metadata": {},
   "source": [
    "After calculating the mean and filling the nulls with it, now we can impute the missing dates in the 'founded_at' column by subtracting the time before first funding from the first funding date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94466ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies['founded_at'].fillna(companies['first_funding_at'] - companies['time_before_first_fund'], inplace = True)\n",
    "companies['founded_at']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463bd9d2",
   "metadata": {},
   "source": [
    "Converting 'aquired_at' of the acquisitons dataset from Object to date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bd74c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "acquis['acquired_at'] =pd.to_datetime(acquis['acquired_at'], errors = 'coerce')\n",
    "acquis.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e58e58",
   "metadata": {},
   "source": [
    "Converting 'funded_at' of the investments dataset from Object to date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5596c659",
   "metadata": {},
   "outputs": [],
   "source": [
    "invests['funded_at'] =pd.to_datetime(invests['funded_at'], errors = 'coerce')\n",
    "invests.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6040d4cf",
   "metadata": {},
   "source": [
    "Converting 'funded_at' of the rounds dataset from Object to date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c2212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds['funded_at'] =pd.to_datetime(rounds['funded_at'], errors = 'coerce')\n",
    "rounds.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c829240",
   "metadata": {},
   "source": [
    "## Imputing Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffe0328",
   "metadata": {},
   "source": [
    "Regarding the category list in the five datasets, since we don't have any indicator on deducing the null values in this column, we will fill the nulls with \"missing\". With samples, it's provided that the same rows (companies) that their category is missing in one dataset, are the same rows in the others so even merging the datasets will not be helpful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13585573",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies['company_category_list'].fillna('missing', inplace=True)\n",
    "acquis['company_category_list'].fillna('missing', inplace=True)\n",
    "acquis['acquirer_category_list'].fillna('missing', inplace=True)\n",
    "invests['company_category_list'].fillna('missing', inplace=True)\n",
    "rounds['company_category_list'].fillna('missing', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47329f2f",
   "metadata": {},
   "source": [
    "## Imputing Raised Amount"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f16a36",
   "metadata": {},
   "source": [
    "There are alot of missing data in the raised amount column in the data set. One strategy that could be taken is calculating the mean of all of the funds of this certain company and impute its nulls with the average of total fund it has recieved before. First we calculate the mean of of the total funding of each company then add it in a new row, then compare each row if the raised amount column is null, then fill it with the average of the company otherwise leave it as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b73b879",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = invests.groupby(['company_name'])['raised_amount_usd'].transform(np.mean)\n",
    "total = pd.DataFrame(total)\n",
    "invests['total_raised'] = total\n",
    "invests['raised_amount_usd'] = np.where((invests['raised_amount_usd'].isnull()), invests['total_raised'], invests['raised_amount_usd'])\n",
    "invests['raised_amount_usd'].fillna(invests['raised_amount_usd'].mean(), inplace = True)\n",
    "invests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4a5fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#farah stop point here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b80a338",
   "metadata": {},
   "source": [
    "## Cleaning & Imputing Geographical Locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50f79a8",
   "metadata": {},
   "source": [
    "### Removing `company_state_code` and `company_region`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63650369",
   "metadata": {},
   "source": [
    "We don't need `company_state_code` in our analysis, <br>\n",
    "As country code and city are sufficient to know <br>\n",
    "an approximation of the geographical location of the company. <br>\n",
    "Therefore, let's remove it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac4b68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies[['company_country_code', 'company_state_code', 'company_city']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f62b94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies.drop('company_state_code', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7974c9",
   "metadata": {},
   "source": [
    "`company_region` is also redundent, as it is usually the same as `company_city`.<br>\n",
    "Let's validate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76581e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: same logic applies for the permalinks\n",
    "acquis.dropna(subset=['acquirer_permalink', 'acquirer_name'], inplace=True)\n",
    "invests.dropna(subset=['investor_permalink', 'investor_name'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754da1f9",
   "metadata": {},
   "source": [
    "Also, we'll remove the missing values for columns that have less than 100 `NaN`, <br>\n",
    "as they're small in comparison to their respective datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3842278",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies[companies.company_region == companies.company_city][['company_region', 'company_city']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb7cf65",
   "metadata": {},
   "source": [
    "Let's see the mean of similarity between the strings using builtin library `difflib.SequenceMatcher`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96082051",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "similarities = []\n",
    "rgs = companies.company_region.values.tolist()\n",
    "cts = companies.company_city.values.tolist()\n",
    "for i in range(min(len(rgs), len(cts))):\n",
    "    if not (rgs[i] != rgs[i] or cts[i] != cts[i]): # If either of the elements is NaN, then don't find the similarity\n",
    "        similarities.append(SequenceMatcher(None, rgs[i], cts[i]).ratio()) # Gets the similarity between each two strings\n",
    "avgSim = sum(similarities) / len(similarities)\n",
    "avgSim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f588b0",
   "metadata": {},
   "source": [
    "However, `SequenceMatcher` only finds the longest contiguous matching subsequence, <br>\n",
    "and all other characters will drastically decrease the similarity, which isn't very accurate, <br>\n",
    "Therefore, let's try to see how many times a string is a substring of another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d02e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsCount = 0\n",
    "for i in range(min(len(rgs), len(cts))):\n",
    "    if not (rgs[i] != rgs[i] or cts[i] != cts[i]):\n",
    "        subsCount += (rgs[i] in cts[i] or cts[i] in rgs[i])\n",
    "subsCount"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3934d5",
   "metadata": {},
   "source": [
    "So to summarize: almost half of the companies have the same data for `region` and `city` (25360) <br>\n",
    "There are around 5000 companies that have the same semantic meaning between the data <br>\n",
    "(e.g: 'New York City' is the same as 'NYC, New York City') <br>\n",
    "Therefore, `company_region` can be removed without losing any possible future insights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cd8d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies.drop('company_region', axis=1, inplace=True)\n",
    "\n",
    "acquis.drop('company_region', axis=1, inplace=True)\n",
    "acquis.drop('acquirer_region', axis=1, inplace=True)\n",
    "\n",
    "invests.drop('company_region', axis=1, inplace=True)\n",
    "invests.drop('investor_region', axis=1, inplace=True)\n",
    "\n",
    "rounds.drop('company_region', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a515f9",
   "metadata": {},
   "source": [
    "### Imputing `company_country_code` and `company_city`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8d74c3",
   "metadata": {},
   "source": [
    "#### Failed attempt using `company_permalink`:\n",
    "1. check the company's permalink\n",
    "2. find HTML that contains the headquarters location (country and city)\n",
    "3. scrape country and convert it to country code using `pycountry.countries`\n",
    "4. scrape city and impute it in `company_city`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cf5dfc",
   "metadata": {},
   "source": [
    "Step 1: appending \"https://www.crunchbase.com\" to each permalink <br>\n",
    "to be able to access the company's webpage on crunchbase. <br>\n",
    "However, since the new permalink will be much longer (thus slower for accessing as a dataset's primary index) <br>\n",
    "we'll create a function that appends the string as a prefix and that will be accessed throughout the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3621c4b5",
   "metadata": {},
   "source": [
    "Step 2: Finding location of country and city on the webpage:\n",
    "let's check company [004](https://www.crunchbase.com/organization/004)'s webpage for example: <Br>\n",
    "<img src=\"Phase 1/countryAndCityInCrunchbase.png\" width=400 height=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8d7cf8",
   "metadata": {},
   "source": [
    "By inspecting the HTML, we notice 4 instances of the class <br>\n",
    "`component--field-formatter field-type-identifier-multi` (in a `span` element),<br>and the data that we want\n",
    "is always the first instance: <br>\n",
    "<img src=\"Phase 1/countryAndCityLocationInHTML.png\" width=700 height=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c8dcd3",
   "metadata": {},
   "source": [
    "Step 3: scrape that info using `requests` and `bs4` libraries: <br>\n",
    "(Note, we're using `threading.Thread` here to increase performance by scraping in multithreads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91270194",
   "metadata": {},
   "source": [
    "However, this is a deadend, as crunchbase is powered by [distill network](https://www.google.com/search?q=distil+network&sxsrf=APq-WBt2mubTHz1xGfmKgpzlWtHah5qAaA%3A1648578960023&ei=kFFDYsx646qVB_nKh7gK&ved=0ahUKEwjMgq-a--v2AhVjVeUKHXnlAacQ4dUDCA4&uact=5&oq=distil+network&gs_lcp=Cgdnd3Mtd2l6EAMyBggAEAcQHjIGCAAQBxAeMgYIABAHEB4yBggAEAcQHjIGCAAQBxAeMgYIABAHEB4yBggAEAcQHjIGCAAQBxAeMgYIABAHEB4yBggAEAcQHjoHCCMQsAMQJzoHCAAQRxCwA0oECEEYAEoECEYYAFCyCliyCmCLD2gBcAF4AIABowGIAaMBkgEDMC4xmAEAoAEByAEKwAEB&sclient=gws-wiz#:~:text=distil%20networks%20uses%20machine%20learning%20to%20identify%20and%20mitigate%20potential%20bad%20bots%2C%20fingerprinting%20them%20so%20that%20they%20can%20still%20be%20tracked%20if%20they%20reconnect%20from%20a%20different%20ip%20address) <br>\n",
    "So when we tried to scrap from it, the following html was always displayed: <br><br>\n",
    "<img src=\"Phase 1/crunchbaseAccessDenied.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d55d854",
   "metadata": {},
   "source": [
    "#### Failed attempt using [linkedin](https://www.linkedin.com/in/ashrafharess/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ec48c8",
   "metadata": {},
   "source": [
    "It failed because after a while, linkedin detects that you are a bot: <br><br>\n",
    "<img src='Phase 1/linkedinSecurityCheck.png' width=500 height=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d091dc5",
   "metadata": {},
   "source": [
    "However, the steps are displayed below to show how this was initially done using multithreads:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a21bd8",
   "metadata": {},
   "source": [
    "Step 1: Login into linkedin using `selenium.webdriver`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2e8502",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # to suppress warnings about internal code deprecations\n",
    "options=webdriver.ChromeOptions()\n",
    "options.add_argument('--incognito')\n",
    "options.add_argument('--headless')\n",
    "driver=webdriver.Chrome(options=options)\n",
    "driver.get('https://www.linkedin.com/uas/login')\n",
    "\n",
    "username = driver.find_element_by_id('username')\n",
    "username.send_keys('xxfarah600xx@gmail.com')\n",
    "password = driver.find_element_by_id('password')\n",
    "password.send_keys('#0LinkedIn0#')\n",
    "log_in_button = driver.find_element_by_class_name('from__button--floating')\n",
    "log_in_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a21bd8",
   "metadata": {},
   "source": [
    "Step 2: Visualize the html of the data you need to scrape: <br><br>\n",
    "<img src='Phase 1/linkedinAboutLocations.png' width=800 height=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda751ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.linkedin.com/company/absolvent/about/')\n",
    "time.sleep(3) # sleeping to render javascript code before parsing to BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e912e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "locCard = soup.find('div', 'org-location-card pv2')\n",
    "locCard.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1603e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "re.findall(r'Primary[\\n\\s]+(\\w+)', locCard.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d15421",
   "metadata": {},
   "source": [
    "Step 3: Visualize another part of the html in case step 3 doesn't work: <br><br>\n",
    "<img src='Phase 1/linkedinAboutHeadquarters.png' width=700 height=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8ec6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.linkedin.com/company/association-for-computing-machinery/about/')\n",
    "time.sleep(3) # sleeping to render javascript code before parsing to BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb28532",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = soup.find('div', 'mb6').find('dl')\n",
    "dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee3a6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = soup.find('div', 'mb6').find('dl')\n",
    "nms = []\n",
    "for tag in dl:\n",
    "    txt = tag.text\n",
    "    nms.append(txt)\n",
    "nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0d5d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the values in key-value pairs, then we will extract the country from key \"Headquarters\"\n",
    "dl = soup.find('div', 'mb6').find('dl')\n",
    "keys = []\n",
    "values = []\n",
    "for tag in dl.find_all('dt'):\n",
    "    txt = tag.text.strip()\n",
    "    if (txt != ''):\n",
    "        keys.append(txt)\n",
    "for tag in dl.find_all('dd'):\n",
    "    txt = tag.text.strip()\n",
    "    if (txt != ''):\n",
    "        values.append(txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9cdd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d5168a",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [txt for txt in values if \"Includes\" not in txt] # a message which contains the word \"Includes\" sometime appears and is not needed, thus we don't include it\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314072a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting the key-value pairs in a dictionary\n",
    "linkedinDict = dict(zip(keys, values))\n",
    "linkedinDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99238f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkedinDict[\"Headquarters\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136f88f5",
   "metadata": {},
   "source": [
    "Step 4: getting the country code of the companies and their corresponding indices and putting them in a list of tuples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be87753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading # using multithreads to speed things up a little\n",
    "lock = threading.Lock()\n",
    "import pycountry\n",
    "from geopy.geocoders import Nominatim # using this library to convert cities to countries\n",
    "import time\n",
    "import re\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"ashraf196280@bue.edu.eg\")\n",
    "\n",
    "idxToNan = []\n",
    "def fetchCountry(idx, link):\n",
    "    try:\n",
    "        lock.acquire()\n",
    "        driver.get(link) # same driver used from step 1\n",
    "        time.sleep(2)\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        lock.release()\n",
    "        dl = soup.find('div', 'mb6').find('dl')\n",
    "        keys = []\n",
    "        values = []\n",
    "        for tag in dl.find_all('dt'):\n",
    "            txt = tag.text.strip()\n",
    "            if (txt != ''):\n",
    "                keys.append(txt)\n",
    "        for tag in dl.find_all('dd'):\n",
    "            txt = tag.text.strip()\n",
    "            if (txt != ''):\n",
    "                values.append(txt)\n",
    "        values = [txt for txt in values if \"Includes\" not in txt]\n",
    "        linkedinDict = dict(zip(keys, values))\n",
    "        if ('Headquarters' in linkedinDict):\n",
    "            city = linkedinDict[\"Headquarters\"]\n",
    "            city = city.split(',')[0] # eg: \"Cupertino, California\" will be [\"Cupertino\", \"California\"], so \"Cupertino\" will be returned\n",
    "        else:\n",
    "            locCard = soup.find('div', 'org-location-card pv2')\n",
    "            city = re.findall(r'Primary[\\n\\s]+(\\w+)', locCard.text)[0]\n",
    "        lock.acquire()\n",
    "        location = geolocator.geocode(city, language=\"en\")\n",
    "        lock.release()\n",
    "        if location is None:\n",
    "            raise\n",
    "    except:\n",
    "        lock.acquire()\n",
    "        idxToNan.append((idx, \"missing\")) # means that this website is not found on linkedin, so declare it as missing\n",
    "        lock.release()\n",
    "        return\n",
    "    \n",
    "    country = location.address.split(', ')[-1]\n",
    "    try:\n",
    "        alpha3Code = pycountry.countries.get(name=country).alpha_3\n",
    "    except:\n",
    "        alpha3Code = country[0:3].upper() #if not found in pycountry, assume that it is the first 3 letters of the country name\n",
    "    lock.acquire()\n",
    "    idxToNan.append((idx, alpha3Code))\n",
    "    lock.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b006652",
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "def imputeFromLinks(df, colToSuffixIntoURL, colToImpute, start, end, linkPrefix = \"\"):\n",
    "    links = df[df[colToImpute].isnull()][[colToSuffixIntoURL]].squeeze()[start:end] # \"squeeze()\" converts df to series\n",
    "    threads = []\n",
    "    for tup in links.iteritems(): # tup[0] --> index, tup[1] --> url (i.e. link)\n",
    "        th = Thread(target=fetchCountry, args=(tup[0], linkPrefix + tup[1].replace(\" \", \"\") + '/about')) #making sure company name doesn't have spaces when suffixed into the url\n",
    "        threads.append(th)\n",
    "        th.start()\n",
    "    for th in threads:\n",
    "        th.join() # to wait until all multithreads finish to properly display the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ded02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single threaded version (just in case multithreads gets blocked by linkedin)\n",
    "from threading import Thread\n",
    "def imputeFromLinksSingleThreads(df, colToSuffixIntoURL, colToImpute, linkPrefix = \"\"):\n",
    "    links = df[df[colToImpute].isnull()][[colToSuffixIntoURL]].squeeze() # \"squeeze()\" converts df to series\n",
    "    for tup in links.iteritems(): # tup[0] --> index, tup[1] --> url (i.e. link)\n",
    "        fetchCountry(tup[0], linkPrefix + tup[1].replace(\" \", \"\") + '/about') #making sure company name doesn't have spaces when suffixed into the url\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e06e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = companies[companies['company_country_code'].isnull()][['company_name']].squeeze()\n",
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa93edfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputeFromLinks(companies, \"company_name\", \"company_country_code\", \"https://www.linkedin.com/company/\", 0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e653b85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#idxToNan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d862db7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this was to save the output to a csv file in case the pc shut off after the above cell executed\n",
    "#dfLinkedin = pd.DataFrame(idxToNan, columns=['index', 'company_country_code'])\n",
    "#dfLinkedin.set_index('index')\n",
    "#dfLinkedin.to_csv('Phase 1/countryCodesLinkedIn.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd3c5a6",
   "metadata": {},
   "source": [
    "#### Successful attempt using TLDs of `homepage_url`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020dd4a8",
   "metadata": {},
   "source": [
    "Let's check the top level domain (TLD) of each link using regex and see if we can use that <br>\n",
    "to impute the country code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd78792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As you can see, there are only approximately 1500 rows that don't contain neither country_code nor homepage_url\n",
    "hUrls = companies[companies['company_country_code'].isnull()][['homepage_url']].squeeze()\n",
    "hUrls.dropna(inplace=True)\n",
    "len(hUrls), len(companies[companies['company_country_code'].isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa0fcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hUrls.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a7d729",
   "metadata": {},
   "outputs": [],
   "source": [
    "hUrls.str.findall(r'(\\.[^.]*)$').head(3) # use '\\.([^.]*)$' if you don't want the '.', but we do, so leave it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fe1a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "hUrls = hUrls.str.findall(r'(\\.[^.]*)$').apply(lambda x : ''.join(x).split('/')[0]) # apply() will convert the list of strings to a string and remove '/' at the end of the string\n",
    "hUrls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0edf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unwantedVals = hUrls.str.contains(pat='^$|com') # \"^$\" means empty string, \"|\" means \"or\"\n",
    "hUrls = hUrls[~unwantedVals] # \"~\" is equivalent to \"unwantedVals == False\"\n",
    "hUrls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f99e5c",
   "metadata": {},
   "source": [
    "Create a dictionary where key = TLD, value = country <br>\n",
    "e.g --> '.jp' : 'Japan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00183320",
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv obtained from https://gist.github.com/derlin/421d2bb55018a1538271227ff6b1299d#file-country-codes-tlds-csv\n",
    "tldsToCountries = pd.read_csv('Phase 1/country-codes-tlds.csv')\n",
    "tldsToCountries.drop_duplicates(subset='tld', keep='first', inplace=True)\n",
    "tldsToCountries['tld'] = tldsToCountries['tld'].apply(lambda x: x.replace(' ', ''))\n",
    "tldsToCountries.to_csv(r'Phase 1/newCountryCodesTlds.csv', index = None, header=True)\n",
    "tldsToCountries = tldsToCountries.set_index('tld').squeeze()\n",
    "tldsToCountries = tldsToCountries.to_dict()\n",
    "tldsToCountries['.jp']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059bb191",
   "metadata": {},
   "source": [
    "Use that dictionary to convert TLDs to their respective countries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc5a236",
   "metadata": {},
   "outputs": [],
   "source": [
    "hCompanyLocs = hUrls.copy()\n",
    "hCompanyLocs = hCompanyLocs.apply(lambda x : tldsToCountries[x] if (x in tldsToCountries) else 'none')\n",
    "hCompanyLocs = hCompanyLocs[hCompanyLocs != 'none']\n",
    "hCompanyLocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a990e0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1213 countries could be imputed\n",
    "len(hCompanyLocs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ed7114",
   "metadata": {},
   "source": [
    "Convert those countries to country codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce29a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry\n",
    "def toAlphaCode3(country):\n",
    "    countryInfo = pycountry.countries.get(name=country)\n",
    "    if (countryInfo is None):\n",
    "        return \"none\"\n",
    "    return countryInfo.alpha_3\n",
    "\n",
    "hCompanyLocs = hCompanyLocs.apply(lambda x : toAlphaCode3(x))\n",
    "hCompanyLocs = hCompanyLocs[hCompanyLocs != 'none']\n",
    "hCompanyLocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fb9660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only 900 countries have country codes present, so impute those only\n",
    "# the following is just to illustrate the for loop in the next cell:\n",
    "hCompanyLocs.index[0], hCompanyLocs.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae27c70",
   "metadata": {},
   "source": [
    "impute into `country_codes` of `companies` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4613f8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(hCompanyLocs)):\n",
    "    idx = hCompanyLocs.index[i]\n",
    "    code = hCompanyLocs.iloc[i]\n",
    "    companies.at[idx, 'company_country_code'] = code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c947b2c1",
   "metadata": {},
   "source": [
    "#### Imputing rest of cities and country codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d028e77a",
   "metadata": {},
   "source": [
    "After the failed attempt to scrape the country and then get the country code of each company we will impute the values with \"missing\" for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f31a0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies['company_country_code'].fillna(\"missing\", inplace=True)\n",
    "companies['company_city'].fillna(\"missing\", inplace=True)\n",
    "\n",
    "acquis['company_country_code'].fillna(\"missing\", inplace=True)\n",
    "acquis['company_state_code'].fillna(\"missing\", inplace=True)\n",
    "acquis['company_city'].fillna(\"missing\")\n",
    "acquis['acquirer_country_code'].fillna(\"missing\", inplace=True)\n",
    "acquis['acquirer_state_code'].fillna(\"missing\", inplace=True)\n",
    "acquis['acquirer_city'].fillna(\"missing\", inplace=True)\n",
    "\n",
    "invests['company_country_code'].fillna(\"missing\", inplace=True)\n",
    "invests['company_state_code'].fillna(\"missing\", inplace=True)\n",
    "invests['company_city'].fillna(\"missing\", inplace=True)\n",
    "invests['investor_city'].fillna(\"missing\", inplace=True)\n",
    "invests['investor_country_code'].fillna(\"missing\", inplace=True)\n",
    "invests['investor_state_code'].fillna(\"missing\", inplace=True)\n",
    "\n",
    "\n",
    "rounds['company_country_code'].fillna(\"missing\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db2e0bd",
   "metadata": {},
   "source": [
    "## Removing duplicates in `acquistions` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251c526e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To show that there really are duplicates in acquisitions dataset\n",
    "acquis[acquis.duplicated(keep = False) == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4087ef7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "acquis.drop_duplicates()\n",
    "acquis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b24876",
   "metadata": {},
   "source": [
    "## Checking and Converting `dtypes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37b517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasetsDtypes(dfList, datasetNames, dtypesCol=\"dtypes_\", pad=\"\"):\n",
    "    cols = []\n",
    "    nulls = []\n",
    "    maxArrayLen = 0\n",
    "    for i, df in enumerate(dfList):\n",
    "        cols.append(df.columns.tolist())\n",
    "        nulls.append(df.dtypes.tolist())\n",
    "        maxArrayLen = max(maxArrayLen, len(cols[i]))\n",
    "    \n",
    "    dictCsvs = {}\n",
    "    for i, df in enumerate(dfList):\n",
    "        cols[i] += [pad] * (maxArrayLen - len(cols[i])) #padding the lists to make them have equal lengths \n",
    "        nulls[i] += [None] *  (maxArrayLen - len(nulls[i]))\n",
    "        dictCsvs[datasetNames[i]] = cols[i]\n",
    "        dictCsvs[dtypesCol + str(i+1)] = nulls[i]\n",
    "    \n",
    "    return pd.DataFrame(dictCsvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12db75af",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetsDtypes(dfAll, lstCsvNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1434c02",
   "metadata": {},
   "source": [
    "From the above dataframe, convert `funding_total_usd` <br>\n",
    "in `companies` datset into float and then fill it and <br>\n",
    "`raised_amount_usd` in `rounds` dataset to  nulls with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f06bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies['funding_total_usd'] = pd.to_numeric(companies['funding_total_usd'],errors = 'coerce')\n",
    "companies['funding_total_usd'].fillna(0, inplace = True)\n",
    "companies['funding_total_usd'] = companies['funding_total_usd'].astype(float, errors = 'raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484b29ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds['raised_amount_usd'].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12db75af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAll = [acquis, companies, invests, rounds] # Reassigning dfAll as sometimes the updates done in a dataframe don't reflect in the list\n",
    "lstCsvNames = [\"acquisitions\", \"companies\", \"investments\", \"rounds\"]\n",
    "datasetsDtypes(dfAll, lstCsvNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da496d35",
   "metadata": {},
   "source": [
    "## Checking Nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6891f616",
   "metadata": {},
   "source": [
    "Now that we imputed all missing data, we will check and see that there are no more null values in any data set. Notice that the other nulls are in columns that are dropped already in the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14968018",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAll = [acquis, companies, invests, rounds] # Reassigning dfAll as sometimes the updates done in a dataframe don't reflect in the list\n",
    "lstCsvNames = [\"acquisitions\", \"companies\", \"investments\", \"rounds\"]\n",
    "dfAllNulls = datasetsNulls(dfAll, lstCsvNames)\n",
    "dfAllNulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3d3955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that \"founded_at\" and \"first_funding_at\" has new NaNs, as the conversion to datetime wasn't successful in 3 rows\n",
    "# so let's remove them\n",
    "companies.dropna(subset=['founded_at'], inplace=True)\n",
    "companies.dropna(subset=['first_funding_at'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d7b22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies.reset_index(inplace = True)\n",
    "companies.drop(['index'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aa6953",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies.dropna(subset = ['homepage_url'], inplace = True)\n",
    "companies.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0ff7f0",
   "metadata": {},
   "source": [
    "# Saving Cleaned Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfce47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prefix = 'datasets/startupsCleaned/'\n",
    "#suffix = 'Cleaned.csv'\n",
    "#acquis.to_csv(prefix+'acquisitions'+suffix, index=False)\n",
    "#companies.to_csv(prefix+'companies'+suffix, index=False)\n",
    "#invests.to_csv(prefix+'investments'+suffix, index=False)\n",
    "#rounds.to_csv(prefix+'rounds'+suffix, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb424a3",
   "metadata": {},
   "source": [
    "# Answering Questions From The Given Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efce33ca",
   "metadata": {},
   "source": [
    "## Question 1 (Ashraf & Farah)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52964d0",
   "metadata": {},
   "source": [
    "-\tCan the factors that affect a start-ups growth be determined?\n",
    "    -\tFor this: We are initially interested in <br> `funding_total_usd`, `status`, and `funding_rounds`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac73f83e",
   "metadata": {},
   "source": [
    "This question can be measured with two approches:\n",
    "- Divide the companies into categories based on their status (either closed, operating or acquired) then look at the average of funding each category receives, then determine the correlation between the closed companies and their failure due to the low funds.\n",
    "- the second approch is finding how frequent a company receives funding regardless of how big or small this funding is. Funding frequency is important to consistently pump money into the company so it is a good indicator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939c2078",
   "metadata": {},
   "source": [
    "First, we will join the two datasets to answer our question which are companies and rounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24adc6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies.sort_values(by=['company_name'], inplace=True)\n",
    "rounds.sort_values(by=['company_name'],inplace=True)\n",
    "comp_rounds = companies.join(rounds.set_index('company_name'), on='company_name', lsuffix='_left', rsuffix='_right')\n",
    "comp_rounds.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5258a2",
   "metadata": {},
   "source": [
    "We will slice our new joined dataset into three categories based on the company's status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc153c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "operatingcomp_rounds = comp_rounds[comp_rounds['status'] == 'operating']\n",
    "operatingcomp_rounds = operatingcomp_rounds[['company_name', 'funding_rounds', 'raised_amount_usd', 'funded_at']]\n",
    "operatingcomp_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba2f1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "acquiredcomp_rounds = comp_rounds[comp_rounds['status'] == 'acquired']\n",
    "acquiredcomp_rounds = acquiredcomp_rounds[['company_name', 'funding_rounds', 'raised_amount_usd', 'funded_at']]\n",
    "acquiredcomp_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b37a13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "closedcomp_rounds = comp_rounds[comp_rounds['status'] == 'closed']\n",
    "closedcomp_rounds = closedcomp_rounds[['company_name', 'funding_rounds', 'raised_amount_usd', 'funded_at']]\n",
    "closedcomp_rounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553e1751",
   "metadata": {},
   "source": [
    "Now we group the operating, acquired, and closed companies by their name and see how much total of fundings across the rounds each company earned. We will see that some of the closed companies did not receive any fundings and this has to be one of the highlighted reasons to their failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4595549",
   "metadata": {},
   "outputs": [],
   "source": [
    "operatingtotal = pd.DataFrame(operatingcomp_rounds.groupby(['company_name'])['raised_amount_usd'].sum())\n",
    "operatingtotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337122b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "acquiredtotal = pd.DataFrame(acquiredcomp_rounds.groupby(['company_name'])['raised_amount_usd'].sum())\n",
    "acquiredtotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32a1191",
   "metadata": {},
   "outputs": [],
   "source": [
    "closedtotal = pd.DataFrame(closedcomp_rounds.groupby(['company_name'])['raised_amount_usd'].sum())\n",
    "closedtotal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc07793",
   "metadata": {},
   "source": [
    " Comparing the raised amount to the operating companies to the closed companies seem to be higher on average. Now let's calculate the average of the whole raised amount to see how much on average does a company need for operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231434bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'status': ['operating', 'aquired', 'closed'], 'mean': [operatingtotal['raised_amount_usd'].mean(), acquiredtotal['raised_amount_usd'].mean(), closedtotal['raised_amount_usd'].mean()]}\n",
    "statuses = pd.DataFrame(d)\n",
    "statuses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10551cc8",
   "metadata": {},
   "source": [
    "We see now on the bar chart that the closed companies received the lowest amount of funding which can prove our intial hypothesis. The operating companies received more amount of funding compared to the closed companies while the acquired companies received much more which makes sense since the acquired companies mostly received those fundings either with debt until they got acquired so they recieved much more than either the closed or the operating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d58d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(statuses['status'], statuses['mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d75714",
   "metadata": {},
   "source": [
    "In Conclusion, not getting enough funding can critically affect the company's chance of success. This indicates that the companies must focus more on finding fundings and investments into their startups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1272896",
   "metadata": {},
   "source": [
    "Regarding the second approach, we will look now at the closed companies that recieved more than one funding round:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765fef4e",
   "metadata": {},
   "source": [
    "Now we will look at the time before the first funding were the fundings for the closed companies. We will ignore the negative signs and consider its absolute values as it might have one date came before the other or it might have been subtracted by 0 in case it received more than one funding on the same day. To solve the negatives problem we will perfom absolute function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89366764",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_rounds = companies.set_index('company_name')\n",
    "comp_rounds['time_before_first_fund'] = comp_rounds['time_before_first_fund'].abs()\n",
    "comp_rounds = comp_rounds.sort_values('time_before_first_fund', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6206c05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "closedcomp_rounds = comp_rounds[comp_rounds['status'] == 'closed']\n",
    "closedcomp_rounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9581bbb4",
   "metadata": {},
   "source": [
    "As seen below and in the above dataframe that the biggest gap between fundingand the foundation was 38653 days which seems to be a significant gap on getting funded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69c3367",
   "metadata": {},
   "outputs": [],
   "source": [
    "operatingcomp_rounds = comp_rounds[comp_rounds['status'] == 'operating']\n",
    "operatingcomp_rounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942b277f",
   "metadata": {},
   "source": [
    "Seeing that the gap between fundingand establishment either the operating companies or the closed ones, it seems the operating companies had bigger gaps. so it may appear that the time between the foundtion and the first funding isn't an effective factor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24c743e",
   "metadata": {},
   "source": [
    "In conclusion, time taken to get a funding can be considered a non-critical factor in affecting the success of the company. It means that once the company rceives the funding nomatter when as long as it's suffcient (based on the last observation) it will get the company operating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ce7bf1",
   "metadata": {},
   "source": [
    "## Question 2 (Ashraf & Farah)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1329d16",
   "metadata": {},
   "source": [
    "Regarding the Second Question:  <br>\n",
    "- \tWhich regions (countries) are most probable to have the most failed startups?\n",
    "    -\tFor this: We are initially interested in <br> `country_code`, `status`, and `investor_name`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1c4eea",
   "metadata": {},
   "source": [
    "First we need to join the companies dataset with the investments dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80861a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies.sort_values(by=['company_name'], inplace=True)\n",
    "invests.sort_values(by=['company_name'],inplace=True)\n",
    "comp_invests = companies.join(invests.set_index('company_name'), on='company_name',lsuffix='_left', rsuffix='_right')\n",
    "comp_invests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e700c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_invests.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b00638",
   "metadata": {},
   "source": [
    "Now, we will categorize the companies according to their status (oprating, closed, aquired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbea43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "closedcomp_invests = comp_invests[comp_invests['status'] == 'closed']\n",
    "closedcomp_invests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be0beaf",
   "metadata": {},
   "source": [
    "We will group by the country to see the number of companies (of each status) in each country to gain insight if there's a massive difference between the number of operating companies and closed ones in a single country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e4be9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "closedcomptotal = pd.DataFrame(closedcomp_invests.groupby(['company_country_code_left'])['company_name'].count())\n",
    "closedcomptotal.drop('missing',inplace=True)\n",
    "closedcomptotal = closedcomptotal.sort_values('company_name', ascending=False)\n",
    "closedcomptotal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8859c7e",
   "metadata": {},
   "source": [
    "Here's a heatmap that illustrates the previous dataframe. It shows us clearly that the US has the most closed companies while others have less. We will take a sample of the first 10 and consider the rest outliers (as their values are 1) because it will be hard to visualize them and their valus won't affect the analysis much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a383661",
   "metadata": {},
   "outputs": [],
   "source": [
    "closedcomptotalsample = closedcomptotal.head(10)\n",
    "sns.heatmap(closedcomptotalsample, vmin=100, vmax=7000);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b3dc7e",
   "metadata": {},
   "source": [
    "Now we categorize the operating companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ff6373",
   "metadata": {},
   "outputs": [],
   "source": [
    "operatingcomp_invests = comp_invests[comp_invests['status'] == 'operating']\n",
    "operatingcomp_invests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec3de2c",
   "metadata": {},
   "source": [
    "We will group by the country and see how many operating companies in each country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7682b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "operatingtotal = pd.DataFrame(operatingcomp_invests.groupby(['company_country_code_left'])['company_name'].count())\n",
    "operatingtotal.drop('missing',inplace=True)\n",
    "operatingtotal = operatingtotal.sort_values('company_name', ascending=False)\n",
    "operatingtotal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dbba17",
   "metadata": {},
   "source": [
    "The heatmap yet again shows that the US has the most operating companies which seems noticable because it also had the most closed companies.We will take a sample of the first 10 and consider the rest outliers (as their values are 1) because it will be hard to visualize them and their valus won't affect the analysis much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0baa5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "operatingtotalsample = operatingtotal.head(10)\n",
    "operatingtotalsample\n",
    "sns.heatmap(operatingtotalsample, vmin=1000, vmax=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514e198d",
   "metadata": {},
   "source": [
    "After looking in the closed and operating companies we see that we still can't determine if the country has more operting companies than the closed or not, so let's take one country and compare. For example, let's look at the USA since it has the highest number of operating companies and closed companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f556fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = comp_invests.loc[comp_invests['company_country_code_left'] == 'USA']\n",
    "df = pd.DataFrame(df.groupby(['status'])['status'].count())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6849d944",
   "metadata": {},
   "source": [
    "The dataframe above shows us how although the US has strong economics and massive amount of investor as seen earlier, it has a bulk of closed companies but compared to operting it's only 0.08 (7956/92228) of it. So, this reflects that The US must have facilities and enough fundings for startups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85009509",
   "metadata": {},
   "source": [
    "If we quickly look at another example and that is the country after the US which is The United Kingdom (GBR) it had 7846 oeprating company with 567 closed companies. Ofcourse, since the US is much bigger the number are biggr but we will look at the ratio between the closed to operating companies to find it 0.07. SIgnificantly it is closed to the US and we can consider that both countries have the same facilities and both show that the lack of investors can affect the companies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e22f85f",
   "metadata": {},
   "source": [
    "In conclusion, as the ratio the ratio between the closed to operating companies increases, the more dificult it is for a company to start up. This will lead us to look closed into factors that makes those countries a difficult place to start a company."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573e7ca4",
   "metadata": {},
   "source": [
    "## Question 3 (Farah)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f283dedb",
   "metadata": {},
   "source": [
    "### How the number of investors in a country can affect the number of successful companies in this country?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78ce02c",
   "metadata": {},
   "source": [
    "First, let's narrow down to the investors that invested in the companies in the same country because there's a posibility that an investor invested in a company in a different country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180718b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_investors = invests[invests['company_country_code'] == invests['investor_country_code']]\n",
    "comp_investors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2bb1c7",
   "metadata": {},
   "source": [
    "Now, let's merge the new dataset with companies so we can get the status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61898218",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_investors = comp_investors.join(companies.set_index('company_name'), on='company_name',lsuffix='_left', rsuffix='_right')\n",
    "comp_investors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b085d91",
   "metadata": {},
   "source": [
    "Then we will categorize our data according to its status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fcaa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "operatingcomp_invests = comp_investors[comp_investors['status'] == 'operating']\n",
    "operatingcomp_invests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699a734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "closedcomp_invests = comp_investors[comp_investors['status'] == 'closed']\n",
    "closedcomp_invests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083550e8",
   "metadata": {},
   "source": [
    "Next, we will group by the investor's country to see how many investors in each country invested in the operating companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439d5afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "operatingcomp_investstotal = pd.DataFrame(operatingcomp_invests.groupby(['investor_country_code'])['investor_name'].count())\n",
    "operatingcomp_investstotal.drop('missing',inplace=True)\n",
    "operatingcomp_investstotal = operatingcomp_investstotal.sort_values('investor_name', ascending=False)\n",
    "operatingcomp_investstotal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde00502",
   "metadata": {},
   "source": [
    "Let's visualize our findinfs through a sample of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea37552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "operatingcomp_investstotal = operatingcomp_investstotal.head(10)\n",
    "sns.heatmap(operatingcomp_investstotal, vmin=100, vmax=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712ff9bb",
   "metadata": {},
   "source": [
    "We will do the same for the closed companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e294d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "closedcomp_investstotal = pd.DataFrame(closedcomp_invests.groupby(['investor_country_code'])['investor_name'].count())\n",
    "closedcomp_investstotal.drop('missing',inplace=True)\n",
    "closedcomp_investstotal = closedcomp_investstotal.sort_values('investor_name', ascending=False)\n",
    "closedcomp_investstotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde2dc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "closedcomp_investstotal = closedcomp_investstotal.head(10)\n",
    "sns.heatmap(closedcomp_investstotal, vmin=10, vmax=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b7c90c",
   "metadata": {},
   "source": [
    "We can see from the data obtained how the number of investors in general in the closed companies is significantlly lower than the those who invested in the operating ones. This can be seen by establishing a ratio between the investors in closed companies to the investors in the operating companies and we will see the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c6c0cc",
   "metadata": {},
   "source": [
    "In conclusion, investing plays an important part in keeping the company operating. As the number of investors increases, the chance of success increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2578332",
   "metadata": {},
   "source": [
    "## Rest of Questions (Aisha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4b3537",
   "metadata": {},
   "source": [
    "Heads-up: \n",
    "1) Here I didn't use the cleaned dataset version, as we can infer some insights from the null values\n",
    "2)Some of the data could have been better if they were represented in charts but due to their data types it wasn't really possible and some of the others with favorable data types had labelling merging with one another making it impossible to read."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cb6a44",
   "metadata": {},
   "source": [
    "### Question 4 : Analyzing Funding Rounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fec57ba",
   "metadata": {},
   "source": [
    "We'll check the number of times funding is repeated overall irrespective of the differenece in company status and then we will see the company statuses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c32842",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### Companies data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c953e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('datasets/startups/companies.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e8acaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['state_code', 'region'],axis=1,inplace=True)\n",
    "df['first_funding_at']=df['first_funding_at'].fillna(\"not available\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c201d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df.status.hist()\n",
    "plt.title('status difference')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ee9013",
   "metadata": {},
   "source": [
    "As the histogram above shows, we have more companies operating as compared to acquired ones which would imply that they received far more funding and produced more than those which were acquired."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4d0a15",
   "metadata": {},
   "source": [
    "#### IPO companies?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93c0afe",
   "metadata": {},
   "source": [
    "It's a situation in which one starts a business by soliciting capital from family, friends, and investors in exchange for a portion of the company. We go to an investor bank and inform them about the idea or product to raise money for this company. They will then introduce us to institutional investors / people who are interested in this idea or product, and they will sell some of the business's shares to assist in bringing the company to the public market. This helps raise a lot of money, sell shares at a reasonable price that suits the shareholder, raise more money through additional rounds of investment, and promote the brand because IPOs deliver a good credibility boost.\n",
    "The disadvantages include:\n",
    "When it comes to matters like openness, the SEC (Securities and Exchange Commission is the US federal agency in charge of regulating and managing financial markets) and exchanges have stringent criteria. As a result, public firms are required to reveal a great deal of business-related information, which may result in competitors obtaining access to material that is the company's trade secret.\n",
    "Less control as shareholders will now affect decision making.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0e4611",
   "metadata": {},
   "source": [
    "We'll now separate companies in to 3 status categories; operating, acquired, and ipo. Then we'll compare the funding rounds based on these statuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19f5a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "operating=df.loc[df['status']=='operating']\n",
    "acquired=df.loc[df['status']=='acquired']\n",
    "ipo=df.loc[df['status']=='ipo']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ed9a7d",
   "metadata": {},
   "source": [
    "#### 1-Operating companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57b85d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "operating.funding_rounds.value_counts().plot(kind=\"bar\",alpha=0.5)\n",
    "plt.title('Funding rounds')\n",
    "plt.xlabel('funding')\n",
    "plt.ylabel('funding frequency') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6a62a1",
   "metadata": {},
   "source": [
    "The above curve can be matched to the values below and we can deduce that operating companies get more one time funding rounds that two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb228cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "operating['funding_rounds'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c424b1c2",
   "metadata": {},
   "source": [
    "Now we will see how this differs from acquired companies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dd617e",
   "metadata": {},
   "source": [
    "#### 2-Acquired companies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d086c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#acquired companies\n",
    "\n",
    "plt.title('funding rounds for aqcuired companies')\n",
    "acquired.funding_rounds.value_counts().plot(kind=\"bar\",alpha=0.5)\n",
    "plt.title('Funding rounds')\n",
    "plt.xlabel('funding')\n",
    "plt.ylabel('funding frequency') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5283a7c1",
   "metadata": {},
   "source": [
    "This can be confirmed and further understood with the values below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60faefca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "acquired['funding_rounds'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8e7f8b",
   "metadata": {},
   "source": [
    "As we can see, both operating and acquired companies get more one time funding rounds than 2 with operating companies having by far the largest fundings. For IPO companies is illustrated below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc48e732",
   "metadata": {},
   "source": [
    "#### 3-IPO companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9fda51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ipo companies\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title('funding rounds for IPO companies')\n",
    "ipo.funding_rounds.value_counts().plot(kind=\"bar\",alpha=0.5)\n",
    "plt.title('Funding rounds')\n",
    "plt.xlabel('funding')\n",
    "plt.ylabel('funding frequency') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb81d562",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ipo['funding_rounds'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b23aed8",
   "metadata": {},
   "source": [
    "We can conclude that operating companies exceed in founding rounds but all have higher one time fundings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990f64b4",
   "metadata": {},
   "source": [
    "#### Investments data set part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361da12f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d=pd.read_csv('datasets/startups/investments.csv')\n",
    "d.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4524409",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d.drop([ 'company_state_code','company_region','investor_state_code','investor_region'],axis=1,inplace=True)\n",
    "d['raised_amount_usd']=d['raised_amount_usd'].fillna(\"not available\")\n",
    "d['funding_round_code']=d['funding_round_code'].fillna(\"not available\")\n",
    "d['company_country_code']=d['company_country_code'].fillna(\"not provided\")\n",
    "d['investor_country_code']=d['investor_country_code'].fillna(\"not provided\")\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d4ebe5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d.drop_duplicates(subset=['company_country_code'])\n",
    "d.drop_duplicates(subset=['investor_country_code'])\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241a9ddb",
   "metadata": {},
   "source": [
    "#### Question 5: How many investors do we have in different countries and which country dominates?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9343ec8",
   "metadata": {},
   "source": [
    "We will do this by checking the amount of times country codes repeat in the investor tables and the company country location names which could tell a little more about the country's financial and business status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8d3638",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.investor_country_code.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84199eb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d.company_country_code.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e757e56",
   "metadata": {},
   "source": [
    "The above data shows that we have more investors in the USA and companies receiving investments as compared to other countries.\n",
    "We can deduce that this country has a better financial and business status as compared to the rest making it appear like pool of gain for investors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec26a95",
   "metadata": {},
   "source": [
    "#### Which investor dominates the market?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e58e17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d['investor_name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c977050",
   "metadata": {},
   "source": [
    "As we can see, Sequoia Capital dominates the market. Wayra and 500 Startups seem to be quit close in count from which we could deduce that they are competitors just like the rest of the companies in the list but the fact that their count have a 4 point gap could mean that they follow up on one another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7d37c8",
   "metadata": {},
   "source": [
    "#### Question 6: Which company is most famous amongst investors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e82e3c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d['company_name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93780c8",
   "metadata": {},
   "source": [
    "The above data shows that the Uber company dominates the market with DocuSign and Fab having the same count which may infere that one is as trust worth as the other. The fact Uber is at the top of the list also tells us that this company has a lot of credibility in the eyes of investors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eec51cc",
   "metadata": {},
   "source": [
    "#### Question 7: What about these company's specialization as compared to investment?Which category (field) attracts more funding? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d8ad1f",
   "metadata": {},
   "source": [
    " We will check the category list for both companies and investments dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50750634",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#companies\n",
    "df = df[df.category_list!= 'NaN']\n",
    "df.category_list.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1e9388",
   "metadata": {},
   "outputs": [],
   "source": [
    "#investments\n",
    "d = d[d.company_category_list!= 'NaN']\n",
    "d.company_category_list.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7d5995",
   "metadata": {},
   "source": [
    "As we can see, although biotechnology is more ofinterest to investors, companies seem to have more interest in software which could be due to expensive and delicate nature of biotechnology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea0a5b5",
   "metadata": {},
   "source": [
    "#### Question 8: Are most of the investors also acquirers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8620111",
   "metadata": {},
   "outputs": [],
   "source": [
    "p=pd.read_csv(\"datasets/startups/acquisitions.csv\")\n",
    "p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f5e96e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p.drop(['company_state_code', 'company_region','acquirer_state_code','acquirer_region'],axis=1,inplace=True)\n",
    "p['company_category_list']=p['company_category_list'].fillna(\"not provided\")\n",
    "p['company_country_code']=p['company_country_code'].fillna(\"not provided\")\n",
    "\n",
    "\n",
    "p.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c55699",
   "metadata": {},
   "source": [
    "Now we'll count the frequency of appearance of acquirer names to determine which one of them is also an investor and compare it to the company names to figure out if the same companies are famous with investors too in this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c04f70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p.acquirer_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8725be",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.company_name.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a7b52b",
   "metadata": {},
   "source": [
    "Shockingly, the first to top the list is Cisco unlike in the investor data set where Sequoia Capital tops the list. Also, it can be noticed that the companies which have had the most share acquisitions are Unveil Technologies and Ufree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb977218",
   "metadata": {},
   "source": [
    "#### Footnote: What can some of the null or missing values infer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c5974b",
   "metadata": {},
   "source": [
    "To do this we'll check the total missing values in the acquisitions data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126a144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da959d10",
   "metadata": {},
   "source": [
    "* The null values in the price_amount column may mean that the acquisition process have not being completed, was cancelled, or the files containing this information are not available.\n",
    "\n",
    "* Null in company_city may point out that either the location of the company is unexact or that the acquisition process was cancelled. The reverse may be true in case with the acquirer_city."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb424a3",
   "metadata": {},
   "source": [
    "# Answering Questions By Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73e9425d",
   "metadata": {},
   "outputs": [],
   "source": [
    "companiesC = pd.read_csv(\"datasets/startupsCleaned/companiesCleaned.csv\", parse_dates=['founded_at', 'first_funding_at', 'last_funding_at', 'time_before_first_fund'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cc69a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import threading\n",
    "from threading import Thread\n",
    "session = requests.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9a12621",
   "metadata": {},
   "outputs": [],
   "source": [
    "lock = threading.Lock()\n",
    "def searchByName(name):\n",
    "    \"\"\"\n",
    "    Searches for the company in the API, and retreives its ID\n",
    "    \"\"\"\n",
    "    url = \"https://app.apollo.io/api/v1/omnisearch/search\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Cookie\": \"zp__initial_utm_source=www.google.com; ZP_Pricing_Split_Test_Variant=21Q3_EC_V49; drift_aid=67cbe002-652d-41f5-8299-25cdf42b3bd3; driftt_aid=67cbe002-652d-41f5-8299-25cdf42b3bd3; remember_token_leadgenie_v2=IjYyNzI0N2Y0MDMyOTA0MDBkOTI3ZjA0NV9sZWFkZ2VuaWVjb29raWVoYXNoIg==--b84a729982761895bd61aaf87c5a9de49f9802dd; app_token=6438fb65adc314e0404230bf41536a2a; zp__utm_source=accounts.google.com; drift_eid=627247f403290400d927f045; __stripe_mid=06eaafd7-76d2-4770-908b-d8f07c1cbebe36217d; GCLB=CJLFtP2rss3o3AE; drift_campaign_refresh=ed98cb61-0a57-4710-9487-473ae481e321; X-CSRF-TOKEN=DvMh51GSNs0pr8bDfPnsZwYMhNmg9yjY+RctJv44cP/CGhK/9YFKq8FTlgBtwqSva3K2ZVboYhTIID5DIpEn9g==; _leadgenie_session=c1RCOGFzSTZuWGsyZjR6TnEwM3RNczZaVXFSREFESFNUWm9XYTZEcElndGZ6My9YbkZVbkpwY1hseEc3bVNCd3BLSkNOWU02UE4xNmJyeWh5QzhpZTFKbnh5Y1dlbFNmQ0NtM0VKckttUXlhMmx5MnFaV3diZllNY3pseExZQk1tcC9vMHFVWGNlS092WlhYWnlHL1J3PT0tLUdqSisxOWRjZkdyRnpHTlZtU1ZhaUE9PQ==--2231905501d53c714778df3cf86d8cbde66e7647\",\n",
    "        \"Origin\": \"https://app.apollo.io\",\n",
    "        \"Referer\": \"https://app.apollo.io/\"\n",
    "    }\n",
    "    data = {\n",
    "        \"query\":f\"{name}\",\n",
    "        \"num_fetch_result\":1,\n",
    "        \"cacheKey\":1650463168797\n",
    "    }\n",
    "    lock.acquire()\n",
    "    response = session.post(url, headers=headers, data=json.dumps(data), timeout=5)\n",
    "    lock.release()\n",
    "    try:\n",
    "        if not len(response.json()['organizations']):\n",
    "            return \"NOT FOUND\"\n",
    "        else:\n",
    "            return response.json()['organizations'][0]['id']\n",
    "    except:\n",
    "        return \"ACCOUNT BLOCKED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6c9e903",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import time\n",
    "lock = threading.Lock()\n",
    "\n",
    "def getCompanies(id):\n",
    "    \"\"\"\n",
    "    Returns Company Info from using the ID retrieved\n",
    "    \"\"\"\n",
    "    if id in [\"NOT FOUND\", \"ACCOUNT BLOCKED\"]:\n",
    "        return \"NOT FOUND\"\n",
    "    else:\n",
    "        url = 'https://app.apollo.io/api/v1/organizations/'+id\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "\n",
    "            \"Cookie\": \"zp__initial_utm_source=www.google.com; ZP_Pricing_Split_Test_Variant=21Q3_EC_V49; drift_aid=67cbe002-652d-41f5-8299-25cdf42b3bd3; driftt_aid=67cbe002-652d-41f5-8299-25cdf42b3bd3; remember_token_leadgenie_v2=IjYyNzI0N2Y0MDMyOTA0MDBkOTI3ZjA0NV9sZWFkZ2VuaWVjb29raWVoYXNoIg==--b84a729982761895bd61aaf87c5a9de49f9802dd; app_token=6438fb65adc314e0404230bf41536a2a; zp__utm_source=accounts.google.com; drift_eid=627247f403290400d927f045; __stripe_mid=06eaafd7-76d2-4770-908b-d8f07c1cbebe36217d; GCLB=CJLFtP2rss3o3AE; drift_campaign_refresh=ed98cb61-0a57-4710-9487-473ae481e321; X-CSRF-TOKEN=DvMh51GSNs0pr8bDfPnsZwYMhNmg9yjY+RctJv44cP/CGhK/9YFKq8FTlgBtwqSva3K2ZVboYhTIID5DIpEn9g==; _leadgenie_session=c1RCOGFzSTZuWGsyZjR6TnEwM3RNczZaVXFSREFESFNUWm9XYTZEcElndGZ6My9YbkZVbkpwY1hseEc3bVNCd3BLSkNOWU02UE4xNmJyeWh5QzhpZTFKbnh5Y1dlbFNmQ0NtM0VKckttUXlhMmx5MnFaV3diZllNY3pseExZQk1tcC9vMHFVWGNlS092WlhYWnlHL1J3PT0tLUdqSisxOWRjZkdyRnpHTlZtU1ZhaUE9PQ==--2231905501d53c714778df3cf86d8cbde66e7647\",\n",
    "            \"Origin\": \"https://app.apollo.io\",\n",
    "            \"Referer\": \"https://app.apollo.io/\"\n",
    "        }\n",
    "        lock.acquire()\n",
    "        response = session.get(url, headers=headers, timeout=5)\n",
    "        lock.release()\n",
    "        #print(response.status_code)\n",
    "        #print(response.text)\n",
    "        return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95add821",
   "metadata": {},
   "outputs": [],
   "source": [
    "companiesC['employee_count'] = 0.0\n",
    "companiesC['country'] = \"\"\n",
    "companiesC['number_of_technologies'] = 0.0\n",
    "companiesC['annual_rev'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ccc5ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = companiesC.loc[0:30000]\n",
    "df2 = companiesC.loc[30001:31000]\n",
    "#df2 = companiesC.loc[30001:61267]\n",
    "dict = {\n",
    "    \"index\" : [],\n",
    "    \"employee_count\" : [],\n",
    "    \"country\" : [],\n",
    "    \"number_of_technologies\" : [],\n",
    "    \"annual_rev\" : []\n",
    "}\n",
    "compInfoTups = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d11bb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from threading import Thread\n",
    "import time\n",
    "lock = threading.Lock()\n",
    "\n",
    "def getCompanyInfo(idx, searchTerm):\n",
    "    res = getCompanies(searchByName(searchTerm))\n",
    "    try:\n",
    "        res = json.loads(res)\n",
    "    except:\n",
    "        print('NO', idx)\n",
    "        return None #usually returns None if getCompanies() returns \"NOT FOUND\"\n",
    "    print('YES', idx)\n",
    "    try:\n",
    "        employee_count = res['organization']['estimated_num_employees']\n",
    "    except:\n",
    "        employee_count = np.NaN\n",
    "    try:\n",
    "        country = res['organization']['country']\n",
    "    except:\n",
    "        country = np.NaN\n",
    "    try:\n",
    "        number_of_technologies = len(res['organization']['technology_names'])\n",
    "    except:\n",
    "        number_of_technologies = np.NaN\n",
    "    try:\n",
    "        annual_rev = res['organization']['annual_revenue']\n",
    "    except:\n",
    "        annual_rev = np.NaN\n",
    "    lock.acquire()\n",
    "    compInfoTups.append((idx, employee_count, country, number_of_technologies, annual_rev))\n",
    "    lock.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b514b299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO 30001\n",
      "YES 30002\n",
      "YES 30003\n",
      "YES 30004\n",
      "YES 30005\n",
      "YES 30006\n",
      "YES 30007\n",
      "YES 30008\n",
      "YES 30009\n",
      "YES 30010\n",
      "YES 30011\n",
      "YES 30012\n",
      "YES 30013\n",
      "YES 30014\n",
      "YES 30015\n",
      "YES 30016\n",
      "YES 30017\n",
      "YES 30018\n",
      "YES 30019\n",
      "YES 30020\n",
      "NO 30028\n",
      "YES 30021\n",
      "YES 30022\n",
      "YES 30023\n",
      "YES 30024\n",
      "YES 30025\n",
      "YES 30026\n",
      "YES 30027\n",
      "YES 30029\n",
      "YES 30030\n",
      "YES 30031\n",
      "YES 30032\n",
      "YES 30033\n",
      "YES 30034\n",
      "YES 30035\n",
      "YES 30036\n",
      "YES 30037\n",
      "YES 30038\n",
      "YES 30039\n",
      "YES 30040\n",
      "NO 30046\n",
      "YES 30041\n",
      "YES 30042\n",
      "YES 30043\n",
      "YES 30044\n",
      "YES 30045\n",
      "YES 30047\n",
      "YES 30048\n",
      "YES 30049\n",
      "YES 30050\n",
      "YES 30051\n",
      "YES 30052\n",
      "YES 30053\n",
      "YES 30054\n",
      "YES 30055\n",
      "YES 30056\n",
      "YES 30057\n",
      "YES 30058\n",
      "YES 30059\n",
      "YES 30060\n",
      "YES 30061\n",
      "YES 30062\n",
      "YES 30063\n",
      "YES 30064\n",
      "YES 30065\n",
      "YES 30066\n",
      "YES 30067\n",
      "YES 30068\n",
      "YES 30069\n",
      "YES 30070\n",
      "NO 30072\n",
      "NO 30075\n",
      "YES 30071\n",
      "YES 30073\n",
      "YES 30074\n",
      "YES 30076\n",
      "YES 30077\n",
      "YES 30078\n",
      "YES 30079\n",
      "YES 30080\n",
      "YES 30081\n",
      "YES 30082\n",
      "YES 30083\n",
      "YES 30084\n",
      "YES 30085\n",
      "YES 30086\n",
      "YES 30087\n",
      "YES 30088\n",
      "YES 30089\n",
      "YES 30090\n",
      "NO 30094\n",
      "YES 30091\n",
      "YES 30092\n",
      "YES 30093\n",
      "YES 30095\n",
      "YES 30096\n",
      "YES 30097\n",
      "YES 30098\n",
      "YES 30099\n",
      "YES 30100\n",
      "YES 30101\n",
      "YES 30102\n",
      "YES 30103\n",
      "YES 30104\n",
      "YES 30105\n",
      "YES 30106\n",
      "YES 30107\n",
      "YES 30108\n",
      "YES 30109\n",
      "YES 30110\n",
      "NO 30116\n",
      "YES 30111\n",
      "YES 30112\n",
      "YES 30113\n",
      "YES 30114\n",
      "YES 30115\n",
      "YES 30117\n",
      "YES 30118\n",
      "YES 30119\n",
      "YES 30120\n",
      "YES 30121\n",
      "YES 30122\n",
      "YES 30123\n",
      "YES 30124\n",
      "YES 30125\n",
      "YES 30126\n",
      "YES 30127\n",
      "YES 30128\n",
      "YES 30129\n",
      "YES 30130\n",
      "YES 30131\n",
      "YES 30132\n",
      "YES 30133\n",
      "YES 30134\n",
      "YES 30135\n",
      "YES 30136\n",
      "YES 30137\n",
      "YES 30138\n",
      "YES 30139\n",
      "YES 30140\n",
      "YES 30141\n",
      "YES 30142\n",
      "YES 30143\n",
      "YES 30144\n",
      "YES 30145\n",
      "YES 30146\n",
      "YES 30147\n",
      "YES 30148\n",
      "YES 30149\n",
      "YES 30150\n",
      "YES 30151\n",
      "YES 30152\n",
      "YES 30153\n",
      "YES 30154\n",
      "YES 30155\n",
      "YES 30156\n",
      "YES 30157\n",
      "YES 30158\n",
      "YES 30159\n",
      "YES 30160\n",
      "YES 30162\n",
      "YES 30161\n",
      "YES 30164\n",
      "YES 30165\n",
      "YES 30166\n",
      "YES 30167\n",
      "YES 30168\n",
      "YES 30169\n",
      "YES 30170\n",
      "YES 30163\n",
      "YES 30171\n",
      "YES 30172\n",
      "YES 30173\n",
      "YES 30174\n",
      "YES 30175\n",
      "YES 30176\n",
      "YES 30177\n",
      "YES 30178\n",
      "YES 30179\n",
      "YES 30180\n",
      "NO 30182\n",
      "YES 30181\n",
      "YES 30183\n",
      "YES 30184\n",
      "YES 30185\n",
      "YES 30186\n",
      "YES 30187\n",
      "YES 30188\n",
      "YES 30189\n",
      "YES 30190\n",
      "NO 30192\n",
      "YES 30191\n",
      "YES 30193\n",
      "YES 30194\n",
      "YES 30195\n",
      "YES 30196\n",
      "YES 30197\n",
      "YES 30198\n",
      "YES 30199\n",
      "YES 30200\n",
      "NO 30204\n",
      "NO 30207\n",
      "YES 30201\n",
      "YES 30202\n",
      "YES 30203\n",
      "YES 30205\n",
      "YES 30206\n",
      "YES 30208\n",
      "YES 30209\n",
      "YES 30210\n",
      "NO 30216\n",
      "YES 30211\n",
      "YES 30212\n",
      "YES 30213\n",
      "YES 30214\n",
      "YES 30215\n",
      "YES 30217\n",
      "YES 30218\n",
      "YES 30219\n",
      "YES 30220\n",
      "YES 30224\n",
      "YES 30221\n",
      "YES 30222\n",
      "YES 30223\n",
      "YES 30226\n",
      "YES 30227\n",
      "YES 30228\n",
      "YES 30229\n",
      "YES 30230\n",
      "YES 30225\n",
      "YES 30239\n",
      "YES 30231\n",
      "YES 30232\n",
      "YES 30233\n",
      "YES 30234\n",
      "YES 30235\n",
      "YES 30236\n",
      "YES 30237\n",
      "YES 30238\n",
      "YES 30240\n",
      "NO 30246\n",
      "YES 30241\n",
      "YES 30242\n",
      "YES 30243\n",
      "YES 30244\n",
      "YES 30245\n",
      "YES 30247\n",
      "YES 30248\n",
      "YES 30249\n",
      "YES 30250\n",
      "YES 30251\n",
      "YES 30252\n",
      "YES 30253\n",
      "YES 30254\n",
      "YES 30255\n",
      "YES 30256\n",
      "YES 30257\n",
      "YES 30258\n",
      "YES 30259\n",
      "YES 30260\n",
      "NO 30267\n",
      "YES 30261\n",
      "YES 30262\n",
      "YES 30263\n",
      "YES 30264\n",
      "YES 30265\n",
      "YES 30266\n",
      "YES 30268\n",
      "YES 30269\n",
      "YES 30270\n",
      "NO 30274\n",
      "NO 30278\n",
      "YES 30271\n",
      "YES 30272\n",
      "YES 30273\n",
      "YES 30275\n",
      "YES 30276\n",
      "YES 30277\n",
      "YES 30279\n",
      "YES 30280\n",
      "NO 30287\n",
      "YES 30281\n",
      "YES 30282\n",
      "YES 30283\n",
      "YES 30284\n",
      "YES 30285\n",
      "YES 30286\n",
      "YES 30288\n",
      "YES 30289\n",
      "YES 30290\n",
      "YES 30291\n",
      "YES 30292\n",
      "YES 30293\n",
      "YES 30294\n",
      "YES 30295\n",
      "YES 30296\n",
      "YES 30297\n",
      "YES 30298\n",
      "YES 30299\n",
      "YES 30300\n",
      "NO 30301\n",
      "YES 30302\n",
      "NO 30310\n",
      "YES 30304\n",
      "YES 30305\n",
      "YES 30306\n",
      "YES 30307\n",
      "YES 30308\n",
      "YES 30309\n",
      "YES 30303\n",
      "NO 30319\n",
      "YES 30311\n",
      "YES 30312\n",
      "YES 30313\n",
      "YES 30314\n",
      "YES 30315\n",
      "YES 30316\n",
      "YES 30317\n",
      "YES 30318\n",
      "YES 30320\n",
      "YES 30321\n",
      "YES 30322\n",
      "YES 30323\n",
      "YES 30324\n",
      "YES 30325\n",
      "YES 30326\n",
      "YES 30327\n",
      "YES 30328\n",
      "YES 30329\n",
      "YES 30330\n",
      "YES 30331\n",
      "YES 30332\n",
      "YES 30333\n",
      "YES 30334\n",
      "YES 30335\n",
      "YES 30336\n",
      "YES 30337\n",
      "YES 30338\n",
      "YES 30339\n",
      "YES 30340\n",
      "YES 30341\n",
      "YES 30342\n",
      "YES 30343\n",
      "YES 30344\n",
      "YES 30345\n",
      "YES 30346\n",
      "YES 30347\n",
      "YES 30348\n",
      "YES 30349\n",
      "YES 30350\n",
      "YES 30351\n",
      "YES 30352\n",
      "YES 30353\n",
      "YES 30354\n",
      "YES 30355\n",
      "YES 30356\n",
      "YES 30357\n",
      "YES 30358\n",
      "YES 30359\n",
      "YES 30360\n",
      "NO 30361\n",
      "NO 30366\n",
      "YES 30362\n",
      "YES 30363\n",
      "YES 30364\n",
      "YES 30365\n",
      "YES 30367\n",
      "YES 30368\n",
      "YES 30369\n",
      "YES 30370\n",
      "YES 30371\n",
      "YES 30372\n",
      "YES 30373\n",
      "YES 30374\n",
      "YES 30375\n",
      "YES 30376\n",
      "YES 30377\n",
      "YES 30378\n",
      "YES 30379\n",
      "YES 30380\n",
      "YES 30381\n",
      "YES 30382\n",
      "YES 30383\n",
      "YES 30384\n",
      "YES 30385\n",
      "YES 30386\n",
      "YES 30387\n",
      "YES 30388\n",
      "YES 30389\n",
      "YES 30390\n",
      "YES 30391\n",
      "YES 30392\n",
      "YES 30393\n",
      "YES 30394\n",
      "YES 30395\n",
      "YES 30396\n",
      "YES 30397\n",
      "YES 30398\n",
      "YES 30399\n",
      "YES 30400\n",
      "YES 30401\n",
      "YES 30402\n",
      "YES 30403\n",
      "YES 30404\n",
      "YES 30405\n",
      "YES 30406\n",
      "YES 30407\n",
      "YES 30408\n",
      "YES 30409\n",
      "YES 30410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-519:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Gamma\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\urllib3\\connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"C:\\Users\\Gamma\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\urllib3\\connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"C:\\Users\\Gamma\\AppData\\Local\\Programs\\Python\\Python38\\lib\\http\\client.py\", line 1344, in getresponse\n",
      "    response.begin()\n",
      "  File \"C:\\Users\\Gamma\\AppData\\Local\\Programs\\Python\\Python38\\lib\\http\\client.py\", line 307, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"C:\\Users\\Gamma\\AppData\\Local\\Programs\\Python\\Python38\\lib\\http\\client.py\", line 268, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"C:\\Users\\Gamma\\AppData\\Local\\Programs\\Python\\Python38\\lib\\socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"C:\\Users\\Gamma\\AppData\\Local\\Programs\\Python\\Python38\\lib\\ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"C:\\Users\\Gamma\\AppData\\Local\\Programs\\Python\\Python38\\lib\\ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Gamma\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\requests\\adapters.py\", line 440, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\Gamma\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\urllib3\\connectionpool.py\", line 785, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\Gamma\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\urllib3\\util\\retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"C:\\Users\\Gamma\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\urllib3\\packages\\six.py\", line 770, in reraise\n",
      "    raise value\n",
      "  File \"C:\\Users\\Gamma\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\Gamma\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\urllib3\\connectionpool.py\", line 451, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"C:\\Users\\Gamma\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\urllib3\\connectionpool.py\", line 340, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='app.apollo.io', port=443): Read timed out. (read timeout=5)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Gamma\\AppData\\Local\\Programs\\Python\\Python38\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Gamma\\AppData\\Local\\Programs\\Python\\Python38\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\Gamma\\AppData\\Local\\Temp\\ipykernel_184632\\1804568253.py\", line 7, in getCompanyInfo\n",
      "  File \"C:\\Users\\Gamma\\AppData\\Local\\Temp\\ipykernel_184632\\3331492044.py\", line 19, in searchByName\n",
      "  File \"C:\\Users\\Gamma\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\requests\\sessions.py\", line 577, in post\n",
      "    return self.request('POST', url, data=data, json=json, **kwargs)\n",
      "  File \"C:\\Users\\Gamma\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\requests\\sessions.py\", line 529, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\Gamma\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\requests\\sessions.py\", line 645, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\Gamma\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\requests\\adapters.py\", line 532, in send\n",
      "    raise ReadTimeout(e, request=request)\n",
      "requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='app.apollo.io', port=443): Read timed out. (read timeout=5)\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    threads = []\n",
    "    try:\n",
    "        for j in range(10):\n",
    "            j += i*10\n",
    "            searchTerm = df2['homepage_url'][j+30001]\n",
    "            th = Thread(target=getCompanyInfo, args=(j+30001, searchTerm))\n",
    "            threads.append(th)\n",
    "            th.start()\n",
    "        for th in threads:\n",
    "            th.join()\n",
    "    finally:\n",
    "        dfTups = pd.DataFrame(compInfoTups, columns=['index', 'employee_count', 'country', 'number_of_technologies', 'annual_rev'])\n",
    "        dfTups.to_csv('Scrapped Companies/scrapped_companies_2.csv', index=False, header=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "22be0740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compInfoTups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb77bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b52815",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for url in range(len(df1)):\n",
    "    res = getCompanies(searchByName(df1['homepage_url'][url]))\n",
    "    emp = re.search(r\"\\\"estimated_num_employees\\\"\n",
    "    :([0-9]+)\" , res)\n",
    "    if emp is not None:\n",
    "        df1['employee_count'][url] = emp.group(1)\n",
    "    elif emp is None:\n",
    "        emp = \"-\"\n",
    "        df1['employee_count'][url] = emp\n",
    "\n",
    "    country = re.search(r\"(\\\"country\\\"\\:)\\\"(\\w+.*)\\\"\\,\\\"o\" , res)\n",
    "    if country is not None:\n",
    "        df1['country'][url] = country.group(2)\n",
    "    elif country is None:\n",
    "        country = \"-\"\n",
    "        df1['country'][url] = country\n",
    "\n",
    "    tech = re.findall(r\"\\\"current_technologies\\\"\\:.*\\]\" , res)\n",
    "    tech = re.findall(r\"\\\"name\\\"\\:\", str(tech))\n",
    "    tech_count = sum('name' in t for t in tech)\n",
    "    df1['number_of_technologies'][url] = tech_count\n",
    "\n",
    "    revenue = re.search(r\"\\\"annual_revenue\\\"\\:([0-9]+)\" , res)\n",
    "    if revenue is not None:\n",
    "        df1['revenue'][url] = revenue.group(1)\n",
    "    elif revenue is None:\n",
    "        revenue = \"-\"\n",
    "        df1['revenue'][url] = revenue\n",
    "\n",
    "df1.to_csv('Scrapped Companies/scrapped_companies.csv', index=False)\n",
    "df1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31deb48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
